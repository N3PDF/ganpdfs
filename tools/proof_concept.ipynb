{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Includes Librarires\n",
    "import lhapdf\n",
    "import math\n",
    "import numpy as np\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build PDF input samples\n",
    "def build_pdf(params, x_grid):\n",
    "    if params['nb_replicas'] < 1:\n",
    "        raise ValueError(\"Number of input replicas must be greater or equal to 1\")\n",
    "    else:\n",
    "        input_size = params['nb_replicas']\n",
    "\n",
    "    pdf_name = params[\"pdf_set_name\"]\n",
    "    flavors  = params['flavors']\n",
    "    q2_scale = params['Q2_scale']\n",
    "\n",
    "    \"\"\"\n",
    "    At the moment, the shape of the input pdf replicas (which gets fed to the\n",
    "    network) is (nb_replicas, flavor, x_grid). This is due to the fact that,\n",
    "    at the end, the generator has to generate as many replicas as on wants\n",
    "    (meaning that the number of replicas has to be in the first argument).\n",
    "\n",
    "    Preferably, one would want to have (nb_flavor, nb_replicas, x_grid).\n",
    "    Maybe there are some ways to do this?\n",
    "    \"\"\"\n",
    "    # If only a single input replicas is given\n",
    "    if input_size == 1:\n",
    "        # Pick the Central Value\n",
    "        pdf = [lhapdf.mkPDF(pdf_name, 0)]\n",
    "    else:\n",
    "        pdf = sample(lhapdf.mkPDFs(pdf_name), input_size)\n",
    "\n",
    "    # Data of Shape (pdf_replicas, flavors, x_grid)\n",
    "    data = []\n",
    "    for p in pdf:\n",
    "        x_space = []\n",
    "        for x in x_grid:\n",
    "            \"\"\"\n",
    "            !!! Here is the crucial part !!!\n",
    "\n",
    "            The model cannot properly learn the distribution of a given flavor\n",
    "            (esp. for the gluon) as the result blows up at small-x. For some\n",
    "            reasons, the model gets stuck and as a result the training does\n",
    "            not improve.\n",
    "\n",
    "            Below, the quarks are represented in terms of their valence. One\n",
    "            possible way to overcome this might be to map the pdf into a much\n",
    "            more manageable function and apply a back transform at the end.\n",
    "            \"\"\"\n",
    "            x_space.append(p.xfxQ2(flavors,x,q2_scale)-p.xfxQ2(-flavors,x,q2_scale))\n",
    "        data.append(x_space)\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cmx\n",
    "from matplotlib.font_manager import FontProperties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Result\n",
    "def plot_input_replicas(data, x_grid):\n",
    "    plt.figure(figsize=[9,6])\n",
    "    for pdf in data:\n",
    "        plt.plot(x_grid, pdf, color='red', alpha=0.45)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('xfx')\n",
    "    plt.grid(True)\n",
    "    plt.xlim([0,1])\n",
    "    plt.ylim([-2e-1,6e-1])\n",
    "    plt.title('Input_replicas')\n",
    "    plt.savefig(\"input_replicas_flavors.png\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Physical Scales\n",
    "input_params = {'pdf_set_name': \"NNPDF31_nnlo_as_0118\", \n",
    "                'flavors': 1,\n",
    "                'Q2_scale': 1.7, \n",
    "                'nb_replicas': 50}\n",
    "                \n",
    "# Toy x_grid\n",
    "x = np.logspace(math.log(1e-5), math.log(1), num=100, base=math.exp(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 100)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_pdf = build_pdf(input_params, x)\n",
    "input_pdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_input_replicas(input_pdf, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test of different GAN Models (as implemented in *src*) for simple Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include libraries for Keras & Tensorflow\n",
    "import os, sys\n",
    "import tensorflow as  tf\n",
    "import keras.backend as K\n",
    "from keras import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Layer\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import LSTM\n",
    "from keras import initializers\n",
    "from keras.layers import Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.layers.advanced_activations import ReLU\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.initializers import RandomNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer derived class for preprocessing\n",
    "class preprocessing_fit(Layer):\n",
    "    \"\"\"\n",
    "    Multiply the previous layer by:\n",
    "            x^a (1-x)^b\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, xval, trainable=True, kernel_initializer='ones', **kwargs):\n",
    "        self.xval = xval\n",
    "        self.trainable = trainable\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        super(preprocessing_fit, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(name='kernel', shape=(2,),\n",
    "                initializer=self.kernel_initializer,\n",
    "                trainable=self.trainable)\n",
    "        super(preprocessing_fit, self).build(input_shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def call(self, pdf):\n",
    "        xres = self.xval**self.kernel[0] * (1-self.xval)**self.kernel[1]\n",
    "        return pdf*xres\n",
    "\n",
    "# Custom layer that contains the information on x_grid\n",
    "class xlayer(Layer):\n",
    "\n",
    "    \"\"\"\n",
    "    Custom array that inputs the information on the x-grid.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_dim, xval, kernel_initializer='glorot_uniform', **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        self.xval = K.constant(xval)\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        super(xlayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(name='kernel', shape=(K.int_shape(self.xval)[0],\n",
    "                input_shape[1], self.output_dim), initializer=self.kernel_initializer,\n",
    "                trainable=True)\n",
    "        super(xlayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        # xres outputs (None, input_shape[1], len(x_pdf))\n",
    "        xres = K.tf.tensordot(x, self.xval, axes=0)\n",
    "        # xfin outputs (None, output_dim)\n",
    "        xfin = K.tf.tensordot(xres, self.kernel, axes=([1,2],[0,1]))\n",
    "        return xfin\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model for Discriminator\n",
    "def discriminator(params, output_size):\n",
    "    # Weights initialization\n",
    "    init  = RandomNormal(stddev=0.02)\n",
    "    # Weights constraint\n",
    "    const = None\n",
    "    # Activation per layer\n",
    "    d_activ = params['d_activ']\n",
    "    d_nodes = params['d_nodes']\n",
    "\n",
    "    ## Critic Architecture ##\n",
    "    D_input = Input(shape=(output_size,))\n",
    "\n",
    "    # 1st hidden dense layer\n",
    "    D_1l = Dense(d_nodes, kernel_initializer=init, kernel_constraint=const)(D_input)\n",
    "    D_1a = d_activ(D_1l)\n",
    "    # 2nd hidden dense layer\n",
    "    D_2l = Dense(d_nodes//2, kernel_initializer=init, kernel_constraint=const)(D_1a)\n",
    "    D_2a = d_activ(D_2l)\n",
    "\n",
    "    # Output 1 dimensional probability\n",
    "    D_output = Dense(1, activation='sigmoid')(D_2a)\n",
    "\n",
    "    model = Model(D_input, D_output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Define Model for Generator\n",
    "def generator(params, output_size):\n",
    "    # Weights initialization\n",
    "    init  = RandomNormal(stddev=0.02)\n",
    "    # Activation per layer\n",
    "    g_activ = params['g_activ']\n",
    "    # noise size and nodes dim\n",
    "    g_nodes     = params['g_nodes']\n",
    "    noise_size  = params['noise_size']\n",
    "\n",
    "    ## Generator Architecture ##\n",
    "    # Input of G/Random noise of 1 dim vector\n",
    "    G_input = Input(shape=(noise_size,))\n",
    "\n",
    "    # 1st hidden dense layer\n",
    "    G_1l = Dense(g_nodes//4, kernel_initializer=init)(G_input)\n",
    "    G_1a = g_activ(G_1l)\n",
    "    if params['x_multiply']:\n",
    "        G_2l = xlayer(g_nodes//2, params['x_grid'], kernel_initializer=init)(G_1a)\n",
    "    else:\n",
    "        G_2l = Dense(g_nodes//2, kernel_initializer=init)(G_1a)\n",
    "    G_2a = g_activ(G_2l)\n",
    "    G_3l = Dense(g_nodes, kernel_initializer=init)(G_2a)\n",
    "    G_3a = g_activ(G_3l)\n",
    "    G_4l = Dense(g_nodes*2, kernel_initializer=init)(G_3a)\n",
    "    G_4a = g_activ(G_4l)\n",
    "    G_5l = Dense(output_size, activation='tanh', kernel_initializer=init)(G_4a)\n",
    "\n",
    "    # Output from the generator\n",
    "    G_output = G_5l\n",
    "    if params['preprocessing']:\n",
    "        G_output = preprocessing_fit(params['x_grid'])(G_5l)\n",
    "\n",
    "    model = Model(G_input, G_output)\n",
    "\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Adversarial Model\n",
    "def adversarial(generator, discriminator):\n",
    "    # make weights in the discriminator not trainable\n",
    "    discriminator.trainable = False\n",
    "\n",
    "    model = Sequential()\n",
    "    # add generator\n",
    "    model.add(generator)\n",
    "    # add the discriminator\n",
    "    model.add(discriminator)\n",
    "    # compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {'g_nodes': 128, 'g_activ': LeakyReLU(0.2), 'noise_size': 100,\n",
    "                'd_nodes': 128, 'd_activ': LeakyReLU(0.2),\n",
    "                'x_multiply': True, 'preprocessing': True, 'x_grid': x}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample input replicas\n",
    "def sample_input_replicas(data, batch_size):\n",
    "    \"\"\" This is a bit trickier as one wants to batch only a subset\n",
    "    of the input replicas while the input data has the following\n",
    "    shape (nb_flavors, nb_replicas, x_grid)\"\"\"\n",
    "\n",
    "    index_replicas = np.random.randint(0, data.shape[0], batch_size)\n",
    "    pdf_batch = data[index_replicas]\n",
    "    # Create labels of ones for reals\n",
    "    y_real = np.ones(batch_size)\n",
    "    return pdf_batch, y_real\n",
    "\n",
    "# generate points in latent space as input for the generator\n",
    "def latent_space(batch_size, noise_size):\n",
    "    # generate points in the latent space\n",
    "    noise_latent = np.random.randn(noise_size * batch_size)\n",
    "    # reshape into a batch of inputs for the network\n",
    "    return noise_latent.reshape(batch_size, noise_size)\n",
    "\n",
    "# Sample output fake replicas\n",
    "def generate_fake_replicas(generator, batch_size, noise_size):\n",
    "    # Feed noise to the generator\n",
    "    noise    = latent_space(batch_size, noise_size)\n",
    "    pdf_fake = generator.predict(noise)\n",
    "    # Create labels of 0 for fake samples\n",
    "    y_fake   = np.zeros(batch_size)\n",
    "    return pdf_fake, y_fake\n",
    "\n",
    "# Sample noise that will enter in the Adv. GAN\n",
    "def sample_input_noise(batch_size, noise_size):\n",
    "    noise = latent_space(batch_size, noise_size)\n",
    "    # Create labels of ones\n",
    "    y_gen = np.ones(batch_size)\n",
    "    return noise, y_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define plot function to summarize the performance\n",
    "def plot_generated_pdf(discriminator, generator, data,\n",
    "                       batch_size, x_grid, noise_size, kth):\n",
    "    path = os.getcwd()\n",
    "    # Create folder\n",
    "    if not os.path.exists('%s/iterations' % path):\n",
    "        try:\n",
    "            original_umask = os.umask(000)\n",
    "            os.makedirs('%s/iterations' % path, 0o777)\n",
    "        finally:\n",
    "            os.umask(original_umask)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    # Prepare real samples\n",
    "    x_real, y_real = sample_input_replicas(data, batch_size)\n",
    "    # Evaluate discriminator on real samples\n",
    "    _, acc_real = discriminator.evaluate(x_real, y_real, verbose=0)\n",
    "    # Prepare fake samples\n",
    "    x_fake, y_fake = generate_fake_replicas(generator, batch_size, noise_size)\n",
    "    # Evaluate discriminator on fake samples\n",
    "    _, acc_fake = discriminator.evaluate(x_fake, y_fake, verbose=0)\n",
    "\n",
    "    plt.figure(figsize=[9,6])\n",
    "    for real, fake in zip(x_real, x_fake):\n",
    "        plt.plot(x_grid, real, color='red', label='real', alpha=0.45)\n",
    "        plt.plot(x_grid, fake, color='blue', label='fake', alpha=0.45)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('xfx')\n",
    "    plt.grid(True)\n",
    "    plt.xlim([0,1])\n",
    "    plt.ylim([-2e-1,2])\n",
    "    plt.title('real_vs_fake_replicas')\n",
    "    plt.savefig(\"%s/iterations/plot_at_iteration_%d.png\" %(path, kth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "def train_gan(disc_model, gen_model, adv_model, x_grid,\n",
    "              data, noise, nb_epochs=100, batch_size=1):\n",
    "    # Batches per epoch\n",
    "    batch_per_epoch = int(data.shape[1]/batch_size)\n",
    "    # Training per iteration\n",
    "    nb_steps = batch_per_epoch * nb_epochs\n",
    "    # Compute half-batch\n",
    "    if batch_size < 2:\n",
    "        half_batch_size = 1\n",
    "    else:\n",
    "        half_batch_size = int(batch_size/2)\n",
    "\n",
    "    # Training iterations\n",
    "    for tr in range(1, nb_steps+1):\n",
    "\n",
    "        # Train the discriminator for a certain steps\n",
    "        for _ in range(1):\n",
    "            # Train on real samples\n",
    "            x_real, y_real = sample_input_replicas(data, half_batch_size)\n",
    "            r_dloss, r_dacc = disc_model.train_on_batch(x_real, y_real)\n",
    "            # Train on fake samples\n",
    "            x_fake, y_fake = generate_fake_replicas(gen_model, half_batch_size, noise)\n",
    "            f_dloss, f_dacc = disc_model.train_on_batch(x_fake, y_fake)\n",
    "        \n",
    "        # Train the Adversarial GAN for a certain setps\n",
    "        for _ in range(1):\n",
    "            # Generate latent point for the generator\n",
    "            inp_noise, y_gen = sample_input_noise(batch_size, noise)\n",
    "            gan_loss = adv_model.train_on_batch(inp_noise, y_gen)\n",
    "        \n",
    "        # Logoutput\n",
    "        if tr % 10000 == 0:\n",
    "            print(\"Iter:{} out of {}. disc_loss: {:6f}. gan_loss: {:6f}\"\n",
    "                   .format(tr, nb_steps+1, r_dloss+f_dloss, gan_loss))\n",
    "            # Generates Plot\n",
    "            plot_generated_pdf(disc_model, gen_model, data, batch_size, x_grid, noise, tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output size\n",
    "output_size = x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disc\n",
    "disc_model = discriminator(model_params, output_size)\n",
    "disc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gen\n",
    "gen_model = generator(model_params, output_size)\n",
    "gen_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adversarial\n",
    "gan_model = adversarial(gen_model, disc_model)\n",
    "gan_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gan(disc_model, gen_model, gan_model, x, input_pdf, model_params['noise_size'], nb_epochs=1000, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ERF Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/wganpdfs')\n",
    "from custom import normalizationK\n",
    "from custom import smm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params  = {'pdf_set_name': \"NNPDF30_nnlo_as_0118\", \n",
    "                'flavors': 1,\n",
    "                'Q2_scale': 1.7, \n",
    "                'nb_replicas': 50}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnpdf30 = build_pdf(test_params, x)\n",
    "nnpdf31 = build_pdf(input_params, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cfd68 = normalizationK(nnpdf31, nnpdf30, 1000)\n",
    "# Take randomized sets from the true\n",
    "rd_nnpdf30 = test_cfd68.random_replicas(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr, fk = test_cfd68.cfd68('mean', rd_nnpdf30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.330885355201029e-30"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cfd68.Nk_mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = {'estimators': ['mean', 'stdev']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "smetric = smm(nnpdf30, nnpdf31, estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.808244437939416e+31"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smetric.ERF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
